---
title: "Econ 144 Project 2"
author: "Shannan Liu, Austin Pham, Zach Wrubel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontfamily: mathpazo
output:
  pdf_document:
    toc: true
  fig_caption: yes
  highlight: haddock
  number_sections: true
  df_print: paged
fontsize: 10.5pt
editor_options:
chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tseries)
library(forecast)
library(stats)
library(stats4)
library(TSA)
library(timeSeries)
library(fUnitRoots)
library(fBasics)
library(tseries)
library(timsac)
library(TTR)
library(strucchange)
library(tis)
library(zoo)
library(gridExtra)
library(ggplot2)
library(vars)
```

\newpage
# I. Introduction  

This project contains on 2 components. First, we will fit forecasting models with a trend, seasonal dummies, and cycles to each of the time series variables. Then, we will fit VAR models to the data and regress each series on the other series and itself.

In part 1 of the project, we aim to determine time dependent models that can capture the stochastic movements of each of the series. In part 2, we investigate whether or not the addition of other regressors can improve the forecasting performance of the purely time dependent models.

## Brief Background on Data:
Sleep Number Corporation (SNBR) is a retail and wholesale company headquartered in Minneapolis that designs, manufactures, and sells a line of air bed mattresses. They provide a variety of beds, bedding, pillows, mattresses, sheets, and duvets for both adults and children. The company also sells other bedding furniture and accessories, and its business is focused on consumers in the United States.

Tupperware Brands Corporation (TUP) is consumer discretionary products company headquartered in Florida. They own a portfolio of global direct selling companies which sell products across multiple brands and categories through an independent sales force. The Company's product brands and categories include food preparation, storage, and serving solutions for the kitchen and home. Tupperware Brands also sells beauty and personal care products.

The S&P 500 Index is a capitalization-weighted index of 505 companies in the US with the highest market capitalization. It is widely regarded as the best gauge of large-cap US equities and serves as a benchmark for many investors. It also includes a wide variety of companies from different industries and sectors, and it captures 80% of total market capitalization in the US.

SNBR and TUP are related to each other because they are both based in the US and sell durable consumer goods (although Tupperware Brands also sells non-durable goods). Therefore, it may be interesting to map our their relationship through the VAR models. These companies are also affected by broader US market movements. Hence, we include the S&P 500 to track that relationship.

## Source of Data
All of the data was sourced from https://finance.yahoo.com/. We will be analyzing weekly observations of the data from December 20, 2010 to February 8, 2022. Company descriptions were adapted from descriptions provided at https://bloomberg.com.


```{r}
# import data
snbr = get.hist.quote("SNBR", start = "2010-12-20", end = "2022-02-08",
                     quote=c("Close"),provider = "yahoo",
                     compression = 'w')
tup = get.hist.quote("TUP", start = "2010-12-20", end = "2022-02-08",
                       quote=c("Close"),provider = "yahoo",
                     compression = 'w')
sp500 = get.hist.quote("^gspc", start = "2010-12-20", end = "2022-02-08",
                       quote=c("Close"),provider = "yahoo",
                     compression = 'w')

# there are no NA values 
cat("Number of NA values in SNBR series:",sum(is.na(snbr)),"\n")
cat("Number of NA values in TUP series:",sum(is.na(tup)),"\n")
cat("Number of NA values in S&P 500 series:",sum(is.na(sp500)))

# create data frame in case we may need it later
df = data.frame(merge(snbr,tup,sp500))
df <- tibble::rownames_to_column(df,"dates")
df$dates<-as.Date(df$dates,"%Y-%m-%d")

# check
head(df)
```

\newpage
# II. Results  

## (A) Produce a time-series plot of your data including the respective ACF and PACF plots.  
```{r}
library(glue)
tsplot <- function(y,series_name) {
  #################################
  # Plot the original time series 
  # A histogram of the series     
  # ACF and PACF plots
  #################################
  ax1 = autoplot.zoo(y, main = sprintf("%s Closing Price",series_name),
         ylab = "Price (USD)",xlab = "Time")
  `Closing Price` = y
  ax2 = gghistogram(`Closing Price`)
  ax3 = ggAcf(y,
              main = glue('{series_name} Sample Autocorrelations'),
              xlab="Displacement",lag.max = 48)
  ax4 = ggPacf(y,
               main = glue('{series_name} Sample Partial Autocorrelations'),
               xlab="Displacement",lag.max = 48)
  grid.arrange(ax1, ax2,ax3,ax4, ncol=2,nrow = 2)
}

# time-series, histogram, ACF and PACF plots
tsplot(snbr,"SNBR")
tsplot(tup,"TUP")
tsplot(sp500,"S&P 500")
```

## (B) Plot the stldecomposition plot of your data, and discuss the results.  
```{r}
# create the time series objects of the closing prices
# set frequency = 52 (roughly 52 weeks in a year)
snbr_ts = ts(snbr$Close, start = c(2010,12,20),frequency = 52)
tup_ts = ts(tup$Close, start = c(2010,12,20),frequency = 52)
sp500_ts = ts(sp500$Close, start = c(2010,12,20),frequency = 52)

autoplot(stl(snbr_ts,s.window = "periodic",robust=TRUE),
     main = "SNBR: STL Decomposition")
autoplot(stl(tup_ts,s.window = "periodic",robust=TRUE),
     main = "TUP STL Decomposition")
autoplot(stl(sp500_ts,s.window = "periodic",robust=TRUE),
     main = "S&P 500 STL Decomposition")
```


Each decomposition shows that the the time series have significant trend, seasonality, and cycle components. Moreover, it shows that each series is not covariance stationary, which we will need to consider when producing models.

SNBR's price trend is mostly upward with some drift at the end of the observed time period. The seasonality of the data looks quite consistent over time, and there are significant cycles in the data, particularly in the last 4 years of the time series.

TUP's price follows a downward trend with some drift. Generally, it fluctuates more than the trend of SNBR's price. The seasonality of the data exhibits more persistence than SNBR's, and it looks consistent over time. The cycles in TUP's price data are very clear and much more significant than what can be observed in SNBR's remainders.

S&P 500's price trend is quite linear and upward sloping. The seasonality of the data looks persistent and consistent over time, and there is significant evidence of cycles in the residuals data, particularly in the last 4 years of the time series.

## (C) & (E): Fit a model that includes, trend, seasonality, and cyclical components. Then plot the ACF and PACF of the respective residuals and interpret the plots.

To capture the complex dynamics of the series, we will be fitting a model with trend, seasonal dummies, and a cycle.

### SNBR Price  
```{r SNBR modelling}
ggtsdisplay(snbr_ts,
            main = "SNBR Price",
            ylab = "Price ($)")
ggtsdisplay(diff(snbr_ts),
            main = "SNBR Price",
            ylab = "Price ($)")
m1 = Arima(snbr_ts,order = c(2,1,2),
           seasonal=list(order=c(1,0,1)),
           method = "CSS")
```
The original series is non-stationary, so we examine the differenced data to produce our model. 

It's quite difficult to tell what process to fit to the differenced data. However, we observe the presence of seasonality and that the strongest low-order spikes in the ACF and PACF graphs stop at the 2nd to 3rd lags. As a result, we propose a ARIMA(2,1,2) + S-ARIMA(1,0,1) process to model the data. 

The trend of the data is taken care of by the I(1) differencing.

Now we look at the ACF and PACF plots of the residuals to see if our model performs well.

```{r}
ggtsdisplay(resid(m1),
            main = "SNBR Model Residuals",
            ylab = "$")
```

The plot of the residuals appears to retain some structure which can be modeled. However, the magnitude of the autocorrelation spikes seen on the ACF and PACF plots suggest that this structure may not be very significant. Therefore, the ARIMA(2,1,2) + S-ARIMA(1,0,1) model fits the data quite well.

### TUP Price  

```{r TUP modelling}
ggtsdisplay(tup_ts,
            main = "TUP Price",
            ylab = "Price ($)")
ggtsdisplay(diff(tup_ts),
            main = "TUP Price",
            ylab = "Price ($)")

m2 <- Arima(tup_ts,order = c(2,1,2))
```

The original series is non-stationary, so we examine the differenced data to produce our model. 

It's quite difficult to tell what process to fit to the differenced data because it looks like white noise. However, we observe that the strongest low-order spikes in the ACF and PACF plots occur at the 2nd lags. As a result, we choose to model the data with an ARIMA(2,1,2) process. 

There is no observable seasonality. Meanwhile, the trend of the data is taken care of by the I(1) differencing.

Now we look at the ACF and PACF plots of the residuals to see if our model performs well.

```{r}
ggtsdisplay(resid(m2),
            main = "TUP Model Residuals",
            ylab = "$")
```

The plot of the residuals appears to have no structure. This intuition is affirmed by the ACF and PACF plots which do not have significant spikes. Thus, the ARIMA(2,1,2) model fits the data quite well.

### S&P 500 Price  

```{r}
ggtsdisplay(sp500_ts,
            main = "TUP Price",
            ylab = "Price ($)")
ggtsdisplay(diff(sp500_ts),
            main = "TUP Price",
            ylab = "Price ($)")

m3 <- Arima(sp500_ts,order = c(5,1,4))
```

The original series is non-stationary, so we examine the differenced data to produce our model. 

It's quite difficult to tell what process to fit to the differenced data because it looks like white noise. However, we observe that the strongest low-order spikes in the ACF and PACF plots occur at the 4th and 5th lags respectively. As a result, we choose to model the data with an ARIMA(5,1,4) process. 

There is no observable seasonality in the differenced data. Meanwhile, the trend of the data is taken care of by the I(1) differencing.

Now we look at the ACF and PACF plots of the residuals to see if our model performs well.

```{r}
ggtsdisplay(resid(m3),
            main = "S&P 500 Model Residuals",
            ylab = "$")
```

The plot of the residuals appears to retain some structure which can be modeled. The residuals are also heteroskedastic. However, the magnitude of the autocorrelation spikes seen on the ACF and PACF plots suggest that this structure may not be very significant. Therefore, the ARIMA(5,1,4) model fits the data quite well.

## (E) Plot the respective residuals vs. fitted values and discuss your observations.

### SNBR Model Residuals vs Fit
```{r}
library(ggplot2)
plot_resid_fit <- function(x,y,title) {
  data = data.frame(x = x, y = y)

  p <- ggplot(data, aes(x =  x, y = y)) +
    geom_smooth(method = "lm", se=T,
                col = "red",
                formula = y ~ x,
                show.legend = T) + 
    geom_point(alpha = 0.5,size = 2,
               col = "black") +
    xlab("Fit") + ylab("Residuals") +
    ggtitle(glue("{title} Residuals vs Fit"))
    
  return(p)
}

plot_resid_fit(fitted(m1),resid(m1),"SNBR")
```

There is not much of a linear relationship between the fit and the residuals, suggesting affirming that our model's fit did pretty well.

### TUP Model Residuals vs Fit  
```{r}
plot_resid_fit(fitted(m2),resid(m2),"TUP")
```

There is basically no linear relationship between the residuals and the fitted data. Therefore, our model for TUP prices did exceptionally well.

### S&P 500 Model Residuals vs Fit  
```{r}
plot_resid_fit(fitted(m3),resid(m3),"S&P 500")
```

There is also no linear relationship between the residuals and the fit of the model fitted to the S&P 500 price series. Therefore, our ARIMA model for this data did very well in fitting the series

## (G) Plot the respective CUSUM and interpret the plot.  

Using the CUSUM test or Cumulative Sum we can test for parameter stability. We hope to ensure that the variance is constant as we add more observations using recursive residuals.

```{r}
plot(efp(m1$res ~ 1, type = "Rec-CUSUM"))
```

According to our recursive CUSUM test, the fluctuation process remains well within the confidence bands meaning that there is no structural break in our first model.

```{r}
plot(efp(m2$res ~ 1, type = "Rec-CUSUM"))
```

According to our recursive CUSUM test, the fluctuation process trends downwards though is still within the confidence bands meaning that there is no structural break in our second model.

```{r}
plot(efp(m3$res ~ 1, type = "Rec-CUSUM"))
```

According to our recursive CUSUM test, the fluctuation process remains well within the confidence bands meaning that there is no structural break in our third model.


## (H) For your model, discuss the associated diagnostic statistics.  

In general, when we look at the ACF/PACF plots for our model's residuals, we see a lack of structure for TUP. For S&P 500, we see a little bit of structure however the magnitude suggests it may be insignificant. We see a little bit more structure for SNBR even at lower orders suggesting we could fit a better model to our series.

In general, the respective residuals vs. fitted values plot lacks any discernible pattern which is good and there is no linear relationship between our residuals and the fit of our model.

Similarly, the recursive CUSUM test shows that all three of our models are structurally sound, meaning there are no breaks in our model specification. 

```{r}
summary(m1)
```
When looking at our model for SNBR, we can see that only the $ar2$ predictor is significant at the $95\%$ confidence level. The lack of more significant predictors demonstrates we may have unnecessary terms in our model.

```{r}
summary(m2)
```
When looking at our model for TUP, we can see that none of the predictors are significant at the $95\%$ confidence level. The lack of more significant predictors demonstrates we may have unnecessary terms in our model.

```{r}
summary(m3)
```
When looking at our model for S&P 500, we can see that only the $ar2$, $ar4$, $ma2$, $ma4$ predictors are significant at the $95\%$ confidence level. The lack of more significant predictors demonstrates we may have unnecessary terms in our model.

## (i) Use your model to forecast 12-steps ahead. Your forecast should include the respective error bands.  

```{r}
autoplot(snbr_ts) + 
  autolayer(forecast(m1, h = 12)) +
  xlab("Year") + ylab("$ Dollars") +
  ggtitle("Forecast for SNBR")
```

```{r}
autoplot(tup_ts) + 
  autolayer(forecast(m2, h = 12)) +
  xlab("Year") + ylab("$ Dollars") +
  ggtitle("Forecast for TUP")
```

```{r}
autoplot(sp500_ts) + 
  autolayer(forecast(m3, h = 12)) +
  xlab("Year") + ylab("$ Dollars") +
  ggtitle("Forecast for TUP")
```

## (j) Compare your forecast from (i) to the 12-steps ahead forecasts from ARIMA, Holt-Winters, and ETS models. Which model performs best in terms of MAPE?  

```{r message=FALSE, warning=FALSE}
snbr_train <- window(snbr_ts, end = c(2021, 9))
m1 <- Arima(snbr_train, order = c(2, 1, 2), seasonal = list(order = c(1, 0, 1)), method = "CSS")
arima1 <- auto.arima(snbr_train)
hw1 <- HoltWinters(snbr_train)
ets1 <- ets(snbr_train)

m_f1 <- forecast(m1, h = 12)
arima_f1 <- forecast(arima1, h = 12)
hw_f1 <- forecast(hw1, h = 12)
ets_f1 <- forecast(ets1, h = 12)

autoplot(snbr_ts) +
  autolayer(m_f1, series="ARIMA", PI=FALSE) +
  autolayer(arima_f1, series="AUTO.ARIMA", PI=FALSE) +
  autolayer(hw_f1, series="HW", PI=FALSE) +
  autolayer(ets_f1, series="ETS", PI=FALSE) +
  xlab("Year") + ylab("$ Dollars") +
  ggtitle("Forecasts for SNBR")

c(ARIMA = accuracy(m_f1, snbr_ts)["Test set","MAPE"],
  AUTO.ARIMA = accuracy(arima_f1, snbr_ts)["Test set","MAPE"],
  HW = accuracy(hw_f1, snbr_ts)["Test set","MAPE"],
  EST = accuracy(ets_f1, snbr_ts)["Test set","MAPE"])
```

In terms of MAPE, AUTO.ARIMA performs best for SNBR. 

```{r message=FALSE, warning=FALSE}
tup_train <- window(tup_ts, end = c(2021, 9))
m2 <-  Arima(tup_train, order = c(2, 1, 2))
arima2 <- auto.arima(tup_train)
hw2 <- HoltWinters(tup_train)
ets2 <- ets(tup_train)

m_f2 <- forecast(m2, h = 12)
arima_f2 <- forecast(arima2, h = 12)
hw_f2 <- forecast(hw2, h = 12)
ets_f2 <- forecast(ets2, h = 12)

autoplot(tup_ts) +
  autolayer(m_f2, series="ARIMA", PI=FALSE) +
  autolayer(arima_f2, series="AUTO.ARIMA", PI=FALSE) +
  autolayer(hw_f2, series="HW", PI=FALSE) +
  autolayer(ets_f2, series="ETS", PI=FALSE) +
  xlab("Year") + ylab("$ Dollars") +
  ggtitle("Forecasts for TUP")

c(ARIMA = accuracy(m_f2, tup_ts)["Test set","MAPE"],
  AUTO.ARIMA = accuracy(arima_f2, tup_ts)["Test set","MAPE"],
  HW = accuracy(hw_f2, tup_ts)["Test set","MAPE"],
  EST = accuracy(ets_f2, tup_ts)["Test set","MAPE"])
```

In terms of MAPE, AUTO.ARIMA performs best for TUP.

```{r message=FALSE, warning=FALSE}
sp500_train <- window(sp500_ts, end = c(2021, 9))
m3 <-  Arima(sp500_train, order = c(5, 1, 4))
arima3 <- auto.arima(sp500_train)
hw3 <- HoltWinters(sp500_train)
ets3 <- ets(sp500_train)

m_f3 <- forecast(m3, h = 12)
arima_f3 <- forecast(arima3, h = 12)
hw_f3 <- forecast(hw3, h = 12)
ets_f3 <- forecast(ets3, h = 12)

autoplot(sp500_ts) +
  autolayer(m_f3, series="ARIMA", PI=FALSE) +
  autolayer(arima_f3, series="AUTO.ARIMA", PI=FALSE) +
  autolayer(hw_f3, series="HW", PI=FALSE) +
  autolayer(ets_f3, series="ETS", PI=FALSE) +
  xlab("Year") + ylab("$ Dollars") +
  ggtitle("Forecasts for S&P500")

c(ARIMA = accuracy(m_f3, sp500_ts)["Test set","MAPE"],
  AUTO.ARIMA = accuracy(arima_f3, sp500_ts)["Test set","MAPE"],
  HW = accuracy(hw_f3, sp500_ts)["Test set","MAPE"],
  EST = accuracy(ets_f3, sp500_ts)["Test set","MAPE"])
```

In terms of MAPE, our original ARIMA performs best for SP500.

## (k) Combine the four forecasts and comment on the MAPE from this forecasts vs., the individual ones.  

```{r}
combo1 <- (m_f1[["mean"]] + arima_f1[["mean"]] + 
  hw_f1[["mean"]] + ets_f1[["mean"]])/4

c(ARIMA = accuracy(m_f1, snbr_ts)["Test set","MAPE"],
  AUTO.ARIMA = accuracy(arima_f1, snbr_ts)["Test set","MAPE"],
  HW = accuracy(hw_f1, snbr_ts)["Test set","MAPE"],
  EST = accuracy(ets_f1, snbr_ts)["Test set","MAPE"],
  COMBINE = accuracy(combo1, snbr_ts)["Test set","MAPE"])
```

The combination performs relatively well in terms of the other models however AUTO.ARIMA still outperforms the rest in terms of MAPE.

```{r}
combo2 <- (m_f2[["mean"]] + arima_f2[["mean"]] + 
  hw_f2[["mean"]] + ets_f2[["mean"]])/4

c(ARIMA = accuracy(m_f2, tup_ts)["Test set","MAPE"],
  AUTO.ARIMA = accuracy(arima_f2, tup_ts)["Test set","MAPE"],
  HW = accuracy(hw_f2, tup_ts)["Test set","MAPE"],
  EST = accuracy(ets_f2, tup_ts)["Test set","MAPE"],
  COMBINE = accuracy(combo2, tup_ts)["Test set","MAPE"])
```

The combination performs relatively well in terms of the other models however AUTO.ARIMA still outperforms the rest in terms of MAPE.

```{r}
combo3 <- (m_f3[["mean"]] + arima_f3[["mean"]] + 
  hw_f3[["mean"]] + ets_f3[["mean"]])/4

c(ARIMA = accuracy(m_f3, sp500_ts)["Test set","MAPE"],
  AUTO.ARIMA = accuracy(arima_f3, sp500_ts)["Test set","MAPE"],
  HW = accuracy(hw_f3, sp500_ts)["Test set","MAPE"],
  EST = accuracy(ets_f3, sp500_ts)["Test set","MAPE"],
  COMBINE = accuracy(combo3, sp500_ts)["Test set","MAPE"])
```

The combination performs relatively well in terms of the other models however our original ARIMA still outperforms the rest in terms of MAPE.


## (l) Fit an appropriate VAR model using your two variables. Make sure to show the relevant plots and discuss your results from the fit.  
```{r}
ccf(snbr_ts,tup_ts,ylab="Cross-Correlation Function", main = "SNBR and TUP CCF")
y=cbind(snbr_ts, tup_ts)
y_tot=data.frame(y)
VARselect(y_tot, lag.max = 10)
VARselect(y_tot, lag.max = 5)
```

When we set the lag max at 5, the VARselect diagnostics agree on a VAR(3) model. But when we set the lag max at 10, the diagnostics do not coincide as much and VARselect results in a VAR(7) model for our two variables. Because of this, we will look at the summary statistics for both a VAR(3) and a VAR(7) model and then choose which to use. 

```{r}
y_model=VAR(y_tot,p=3)
y_model2=VAR(y_tot,p=7)
summary(y_model2)
summary(y_model)
```

From comparing the RSE and the Adjusted R-squared, we find the VAR(7) model is a better fit. Now we will look at the ACF and PACF of the residuals from our VAR(7) fit.

```{r}
plot(y_model2)
tsdisplay(residuals(y_model2)[,2])
```

The residuals resemble white noise and the scale of the PACF suggests that there are not significant resulting lags and thus the VAR(7) takes care of all of the dynamics and is a good fit for this series.

## (m) Compute, plot, and interpret the respective impulse response functions.  
```{r}
quartz()
irf(y_model2)
plot(irf(y_model2, n.ahead=36))
```


## (n) Perform a Granger-Causality test on your variables and discuss your results from the test.  
```{r}
grangertest(snbr_ts ~ tup_ts, order = 7)
```



```{r}
grangertest(tup_ts ~ snbr_ts, order = 7)
```


## (o) Use your VAR model to forecast 12-steps ahead. Your forecast should include the respective error bands. Comment on the differences between the VAR forecast and the other ones obtained using the different methods.  
```{r}
var.predict = predict(object=y_model2, n.ahead=12)
quartz()
plot(var.predict)
plot(fevd(y_model2, n.ahead = 12))
plot(stability(y_model2, type = "Rec-CUSUM"), plot.type="single")
```


\newpage
# III. Conclusions and Future Work  

\newpage
# IV. References  

\newpage
# V. R Source Code  
